{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "loss: 2.398561\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.classifiers.softmax import softmax_loss_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0)\n",
    "f = lambda w: softmax_loss_vectorized(w, X_dev, y_dev, 0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "tic = time.time()\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 401.026842s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bc17e1eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "#from cs231n.classifiers import LinearClassifier\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "import time\n",
    "\n",
    "svm =  Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-6, reg=2.5e4, num_iters=10000, verbose=False)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "\n",
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.subplot(211)\n",
    "plt.semilogx(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 1.00e-07, regularization is 2.50e+04\n",
      "Train accuracy is 0.29806122449  validation accuracy is  0.309\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-07, regularization is 2.78e+04\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "learning_rates = [-7, -6]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.4 on the validation set.\n",
    "leRange = np.logspace(learning_rates[0],learning_rates[1], 10)\n",
    "reRange = np.linspace(regularization_strengths[0],regularization_strengths[1], 10)\n",
    "\n",
    "\n",
    "for le in leRange:\n",
    "    for reg in reRange:\n",
    "        print('Learning rate is %4.2e, regularization is %4.2e' % (le, reg))\n",
    "        svm = Softmax()\n",
    "        svm.train(X_train, y_train, learning_rate=le, reg=reg, num_iters=5000)\n",
    "        \n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        \n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        print('Train accuracy is', train_acc,' validation accuracy is ', val_acc)\n",
    "        print('\\n')\n",
    "        \n",
    "        results.update({(le,reg):(train_acc,val_acc)})\n",
    "        \n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = svm\n",
    "                \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 5\n",
    "(cls == cl).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231_py3",
   "language": "python",
   "name": "cs231_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
