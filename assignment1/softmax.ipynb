{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "loss: 2.327567\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ff03c88c04db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Complete the implementation of softmax_loss_naive and implement a (naive)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# version of the gradient that uses nested loops.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_loss_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# As we did for the SVM, use numeric gradient checking as a debugging tool.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\work\\cs231\\assignment1\\cs231n\\classifiers\\softmax.py\u001b[0m in \u001b[0;36msoftmax_loss_naive\u001b[1;34m(W, X, y, reg)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# features loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# dw = X[k,i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m/=\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 20)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.122730 analytic: -0.122730, relative error: 1.618643e-07\n",
      "numerical: -0.011118 analytic: -0.011118, relative error: 3.140105e-06\n",
      "numerical: 0.804930 analytic: 0.804930, relative error: 1.669996e-09\n",
      "numerical: -1.467934 analytic: -1.467934, relative error: 7.923331e-09\n",
      "numerical: 0.751231 analytic: 0.751231, relative error: 3.919346e-08\n",
      "numerical: 0.000635 analytic: 0.000635, relative error: 6.714847e-05\n",
      "numerical: 0.221358 analytic: 0.221358, relative error: 2.070731e-07\n",
      "numerical: 1.395881 analytic: 1.395881, relative error: 9.924403e-09\n",
      "numerical: 0.292291 analytic: 0.292291, relative error: 2.581070e-08\n",
      "numerical: -1.287130 analytic: -1.287130, relative error: 3.117381e-08\n"
     ]
    }
   ],
   "source": [
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0)\n",
    "f = lambda w: softmax_loss_vectorized(w, X_dev, y_dev, 0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0035097599029541016\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "tic = time.time()\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "naive loss: 2.327567e+00 computed in 12.374160s\n",
      "vectorized loss: 2.327567e+00 computed in 0.003008s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 50000: loss 777.065639\n",
      "iteration 1000 / 50000: loss 763.472353\n",
      "iteration 2000 / 50000: loss 756.374129\n",
      "iteration 3000 / 50000: loss 750.952523\n",
      "iteration 4000 / 50000: loss 746.150627\n",
      "iteration 5000 / 50000: loss 742.254986\n",
      "iteration 6000 / 50000: loss 738.329988\n",
      "iteration 7000 / 50000: loss 734.972597\n",
      "iteration 8000 / 50000: loss 731.839834\n",
      "iteration 9000 / 50000: loss 728.953765\n",
      "iteration 10000 / 50000: loss 726.225250\n",
      "iteration 11000 / 50000: loss 723.663935\n",
      "iteration 12000 / 50000: loss 721.399200\n",
      "iteration 13000 / 50000: loss 719.107626\n",
      "iteration 14000 / 50000: loss 716.782887\n",
      "iteration 15000 / 50000: loss 714.827802\n",
      "iteration 16000 / 50000: loss 712.564630\n",
      "iteration 17000 / 50000: loss 710.890697\n",
      "iteration 18000 / 50000: loss 708.860945\n",
      "iteration 19000 / 50000: loss 707.139210\n",
      "iteration 20000 / 50000: loss 705.496214\n",
      "iteration 21000 / 50000: loss 703.754342\n",
      "iteration 22000 / 50000: loss 702.276216\n",
      "iteration 23000 / 50000: loss 700.569258\n",
      "iteration 24000 / 50000: loss 699.179392\n",
      "iteration 25000 / 50000: loss 697.763344\n",
      "iteration 26000 / 50000: loss 696.438069\n",
      "iteration 27000 / 50000: loss 695.043940\n",
      "iteration 28000 / 50000: loss 693.806299\n",
      "iteration 29000 / 50000: loss 692.470389\n",
      "iteration 30000 / 50000: loss 691.241775\n",
      "iteration 31000 / 50000: loss 689.781132\n",
      "iteration 32000 / 50000: loss 688.893505\n",
      "iteration 33000 / 50000: loss 687.640547\n",
      "iteration 34000 / 50000: loss 686.460501\n",
      "iteration 35000 / 50000: loss 685.342266\n",
      "iteration 36000 / 50000: loss 684.392433\n",
      "iteration 37000 / 50000: loss 683.374547\n",
      "iteration 38000 / 50000: loss 682.245056\n",
      "iteration 39000 / 50000: loss 681.014361\n",
      "iteration 40000 / 50000: loss 680.166187\n",
      "iteration 41000 / 50000: loss 679.134480\n",
      "iteration 42000 / 50000: loss 678.296207\n",
      "iteration 43000 / 50000: loss 677.443247\n",
      "iteration 44000 / 50000: loss 676.308912\n",
      "iteration 45000 / 50000: loss 675.399750\n",
      "iteration 46000 / 50000: loss 674.654279\n",
      "iteration 47000 / 50000: loss 673.855539\n",
      "iteration 48000 / 50000: loss 672.985304\n",
      "iteration 49000 / 50000: loss 672.104823\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAHnCAYAAAD96vpUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x242d0dddbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=50000, verbose=True)\n",
    "toc = time.time()\n",
    "#print('That took %fs' % (toc - tic))\n",
    "\n",
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.semilogx(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 1.00e-06, regularization is 1.00e+04\n",
      "Train accuracy is 0.4193, validation accuracy is 0.3660\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 2.00e+04\n",
      "Train accuracy is 0.4213, validation accuracy is 0.3830\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 3.00e+04\n",
      "Train accuracy is 0.4254, validation accuracy is 0.3770\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 4.00e+04\n",
      "Train accuracy is 0.4223, validation accuracy is 0.3850\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 5.00e+04\n",
      "Train accuracy is 0.4228, validation accuracy is 0.3580\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 6.00e+04\n",
      "Train accuracy is 0.4226, validation accuracy is 0.3830\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 7.00e+04\n",
      "Train accuracy is 0.4206, validation accuracy is 0.3810\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 8.00e+04\n",
      "Train accuracy is 0.4247, validation accuracy is 0.3940\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 9.00e+04\n",
      "Train accuracy is 0.4217, validation accuracy is 0.3820\n",
      "\n",
      "\n",
      "Learning rate is 1.00e-06, regularization is 1.00e+05\n",
      "Train accuracy is 0.4223, validation accuracy is 0.3740\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 1.00e+04\n",
      "Train accuracy is 0.4346, validation accuracy is 0.3900\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 2.00e+04\n",
      "Train accuracy is 0.4357, validation accuracy is 0.3910\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 3.00e+04\n",
      "Train accuracy is 0.4317, validation accuracy is 0.4000\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 4.00e+04\n",
      "Train accuracy is 0.4286, validation accuracy is 0.3750\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 5.00e+04\n",
      "Train accuracy is 0.4324, validation accuracy is 0.3710\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 6.00e+04\n",
      "Train accuracy is 0.4328, validation accuracy is 0.3850\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 7.00e+04\n",
      "Train accuracy is 0.4308, validation accuracy is 0.3960\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 8.00e+04\n",
      "Train accuracy is 0.4338, validation accuracy is 0.3870\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 9.00e+04\n",
      "Train accuracy is 0.4294, validation accuracy is 0.3880\n",
      "\n",
      "\n",
      "Learning rate is 1.67e-06, regularization is 1.00e+05\n",
      "Train accuracy is 0.4275, validation accuracy is 0.4060\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 1.00e+04\n",
      "Train accuracy is 0.4291, validation accuracy is 0.3820\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 2.00e+04\n",
      "Train accuracy is 0.4404, validation accuracy is 0.3680\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 3.00e+04\n",
      "Train accuracy is 0.4366, validation accuracy is 0.3840\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 4.00e+04\n",
      "Train accuracy is 0.4380, validation accuracy is 0.3840\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 5.00e+04\n",
      "Train accuracy is 0.4326, validation accuracy is 0.3710\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 6.00e+04\n",
      "Train accuracy is 0.4419, validation accuracy is 0.4010\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 7.00e+04\n",
      "Train accuracy is 0.4419, validation accuracy is 0.4020\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 8.00e+04\n",
      "Train accuracy is 0.4389, validation accuracy is 0.3680\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 9.00e+04\n",
      "Train accuracy is 0.4351, validation accuracy is 0.3850\n",
      "\n",
      "\n",
      "Learning rate is 2.78e-06, regularization is 1.00e+05\n",
      "Train accuracy is 0.4322, validation accuracy is 0.3720\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 1.00e+04\n",
      "Train accuracy is 0.4358, validation accuracy is 0.3810\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 2.00e+04\n",
      "Train accuracy is 0.4061, validation accuracy is 0.3520\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 3.00e+04\n",
      "Train accuracy is 0.4247, validation accuracy is 0.3610\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 4.00e+04\n",
      "Train accuracy is 0.4369, validation accuracy is 0.3720\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 5.00e+04\n",
      "Train accuracy is 0.4370, validation accuracy is 0.3770\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 6.00e+04\n",
      "Train accuracy is 0.4205, validation accuracy is 0.3670\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 7.00e+04\n",
      "Train accuracy is 0.4467, validation accuracy is 0.3960\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 8.00e+04\n",
      "Train accuracy is 0.4327, validation accuracy is 0.3860\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 9.00e+04\n",
      "Train accuracy is 0.4423, validation accuracy is 0.3790\n",
      "\n",
      "\n",
      "Learning rate is 4.64e-06, regularization is 1.00e+05\n",
      "Train accuracy is 0.4392, validation accuracy is 0.4020\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 1.00e+04\n",
      "Train accuracy is 0.3687, validation accuracy is 0.3120\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 2.00e+04\n",
      "Train accuracy is 0.4035, validation accuracy is 0.3600\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 3.00e+04\n",
      "Train accuracy is 0.3557, validation accuracy is 0.3140\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 4.00e+04\n",
      "Train accuracy is 0.3872, validation accuracy is 0.3290\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 5.00e+04\n",
      "Train accuracy is 0.3818, validation accuracy is 0.3340\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 6.00e+04\n",
      "Train accuracy is 0.3831, validation accuracy is 0.3330\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 7.00e+04\n",
      "Train accuracy is 0.3234, validation accuracy is 0.2890\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 8.00e+04\n",
      "Train accuracy is 0.3451, validation accuracy is 0.3150\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 9.00e+04\n",
      "Train accuracy is 0.3820, validation accuracy is 0.3180\n",
      "\n",
      "\n",
      "Learning rate is 7.74e-06, regularization is 1.00e+05\n",
      "Train accuracy is 0.3900, validation accuracy is 0.3370\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 1.00e+04\n",
      "Train accuracy is 0.3943, validation accuracy is 0.3360\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 2.00e+04\n",
      "Train accuracy is 0.3321, validation accuracy is 0.2780\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 3.00e+04\n",
      "Train accuracy is 0.3443, validation accuracy is 0.3150\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 4.00e+04\n",
      "Train accuracy is 0.3461, validation accuracy is 0.2850\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 5.00e+04\n",
      "Train accuracy is 0.3369, validation accuracy is 0.3000\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 6.00e+04\n",
      "Train accuracy is 0.2933, validation accuracy is 0.2580\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 7.00e+04\n",
      "Train accuracy is 0.3586, validation accuracy is 0.3210\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 8.00e+04\n",
      "Train accuracy is 0.3412, validation accuracy is 0.3130\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 9.00e+04\n",
      "Train accuracy is 0.3483, validation accuracy is 0.3200\n",
      "\n",
      "\n",
      "Learning rate is 1.29e-05, regularization is 1.00e+05\n",
      "Train accuracy is 0.3403, validation accuracy is 0.2940\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 1.00e+04\n",
      "Train accuracy is 0.3370, validation accuracy is 0.3080\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 2.00e+04\n",
      "Train accuracy is 0.2808, validation accuracy is 0.2690\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 3.00e+04\n",
      "Train accuracy is 0.3469, validation accuracy is 0.3110\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 4.00e+04\n",
      "Train accuracy is 0.3372, validation accuracy is 0.3040\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 5.00e+04\n",
      "Train accuracy is 0.3063, validation accuracy is 0.2600\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 6.00e+04\n",
      "Train accuracy is 0.3406, validation accuracy is 0.2850\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 7.00e+04\n",
      "Train accuracy is 0.3008, validation accuracy is 0.2750\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 8.00e+04\n",
      "Train accuracy is 0.3128, validation accuracy is 0.2720\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 9.00e+04\n",
      "Train accuracy is 0.2928, validation accuracy is 0.2690\n",
      "\n",
      "\n",
      "Learning rate is 2.15e-05, regularization is 1.00e+05\n",
      "Train accuracy is 0.3881, validation accuracy is 0.3170\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 1.00e+04\n",
      "Train accuracy is 0.3059, validation accuracy is 0.2810\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 2.00e+04\n",
      "Train accuracy is 0.3264, validation accuracy is 0.2780\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 3.00e+04\n",
      "Train accuracy is 0.3756, validation accuracy is 0.3450\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 4.00e+04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.3234, validation accuracy is 0.2750\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 5.00e+04\n",
      "Train accuracy is 0.3118, validation accuracy is 0.2920\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 6.00e+04\n",
      "Train accuracy is 0.3551, validation accuracy is 0.3130\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 7.00e+04\n",
      "Train accuracy is 0.3190, validation accuracy is 0.2770\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 8.00e+04\n",
      "Train accuracy is 0.2940, validation accuracy is 0.2650\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 9.00e+04\n",
      "Train accuracy is 0.3268, validation accuracy is 0.3210\n",
      "\n",
      "\n",
      "Learning rate is 3.59e-05, regularization is 1.00e+05\n",
      "Train accuracy is 0.3043, validation accuracy is 0.2880\n",
      "\n",
      "\n",
      "Learning rate is 5.99e-05, regularization is 1.00e+04\n",
      "Train accuracy is 0.3263, validation accuracy is 0.3150\n",
      "\n",
      "\n",
      "Learning rate is 5.99e-05, regularization is 2.00e+04\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [-6, -4]\n",
    "regularization_strengths = [1e4, 1e5]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "leRange = np.logspace(learning_rates[0],learning_rates[1], 10)\n",
    "reRange = np.linspace(regularization_strengths[0],regularization_strengths[1], 10)\n",
    "\n",
    "for le in leRange:\n",
    "    for reg in reRange:\n",
    "        print('Learning rate is %4.2e, regularization is %4.2e' % (le, reg))\n",
    "        svm = Softmax()\n",
    "        svm.train(X_train, y_train, learning_rate=le, reg=reg, num_iters=10000)\n",
    "        \n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        \n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        print(f'Train accuracy is {train_acc:.4f}, validation accuracy is {val_acc:.4f}')\n",
    "        print('\\n')\n",
    "        \n",
    "        results.update({(le,reg):(train_acc,val_acc)})\n",
    "        \n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_svm = svm\n",
    "                \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Learning rate is 4.64e-06, regularization is 1.00e+05\n",
    "Train accuracy is 0.4392, validation accuracy is 0.4020\n",
    "\n",
    "Learning rate is 2.78e-06, regularization is 7.00e+04\n",
    "Train accuracy is 0.4419, validation accuracy is 0.4020\n",
    "\n",
    "Learning rate is 1.67e-06, regularization is 1.00e+05\n",
    "Train accuracy is 0.4275, validation accuracy is 0.4060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
